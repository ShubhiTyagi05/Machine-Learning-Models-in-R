#####1. INTRO TO R#####

#Intro to r

#Dataframes, R dataset mtcars
mtcars
df<-mtcars

#Dimensions of data frame
dim(df)

#Number of rows and columns
cols<-ncol(df)
nrow(df)

#Understanding structure of dataframe 
str(df)

#Checking top 5 rows of dataframe
head(df,5)

#Extracting one column from dataframe
df$mpg

#Check type of variable
typeof(df$mpg)

#Summary
summary(df$mpg)
summary(df)

#Selcting subset of rows from dataframe
subRow <- df[5:15,]
subRow
#Selecting subset of coumns from dataframe
subCol<- mtcars[,2:4]
#Select subset of non conttiguous columns
subColNonConti<- mtcars[, c(2:4,6:9,11)]
subColNonConti
#Selecting subset by removing certain columns
subColRem<- mtcars[,-c(2:4,6)]
subColRem

subCol<- mtcars[,c('cyl','mpg')]
subCol


#############################################################################

setwd("C:/Users/Shubhi Tyagi/Downloads/Analytics Academy/Batch2/Class Material/Session 2")

#install a library (one time)
install.packages("xlsx")

#Calling a library (every time for new R session)
library(xlsx)

#read xlsx file and specify sheet number
mtcars1<-read.xlsx("MtCars.xlsx",1)
mtcars1

mtcars2<-read.xlsx("MtCars2.xlsx",1)
mtcars2

#merge 2 datasets
mtcarsFull <- rbind(mtcars1,mtcars2)

mtcarsFull

#order dataset by desired feature in descending order
mtcarsFull<- mtcarsFull[order(-mtcarsFull$mpg),]

mtcarsFull

#Statistical summary of variabes
summary(mtcarsFull)

#analyse vehicles with mpg>20, filtering
mtcarsHighMpg <- mtcarsFull[mtcarsFull$mpg>20,]

mtcarsHighMpg

# check proportion of cylinders in high mpg cars, frequency distribution of categorical variables
counts<- table(mtcarsHighMpg$cyl)
counts

#pie chart
pie(counts, main="Pie Chart of Cylinders")

mean(mtcarsHighMpg$mpg)

#Scatter plot, mpg vs disp
plot(mtcars$mpg,mtcars$disp)

mean(mtcarsFull$mpg)

# check proportion of cylinders in all the cars
counts<- table(mtcarsFull$cyl)
counts

#pie chart
jpeg('rplot3.jpg')
#pie chart
pie(counts, main="Pie Chart of Cylinders")
dev.off()

###################################################################################################################################
#####2. INTRO TO STATS#####
###################################################################################################################################


#Reading : https://towardsdatascience.com/data-science-simplified-hypothesis-testing-56e180ef2f71


install.packages("plotrix")
install.packages("plyr")
install.packages("gridExtra")
install.packages("ggplot2")

#Descriptive statistics

str(mtcars)
dim(mtcars)


#Type of variables
#Categorical
summary(mtcars$gear)
#as.factor(mtcars$gear)

#check distribution with bar plot
counts<- table(mtcars$gear)
counts

barplot(counts, main="Car Distribution", 
        xlab="Gears")

#pie chart
pie(counts, main="Pie Chart of Gears")

#3D pie chart

library(plotrix)
label<-  paste(names(counts), "\n",counts, sep="")
pie3D(counts, labels = label,explode=0.1,
      main="Pie Chart of Gears ")


#Mode of categorical variables, get as Dataframe not as table

library(plyr)
countSpecies = count(mtcars, "gear")
countSpecies

#Numeric

str(mtcars)
summary(mtcars$mpg)
median(mtcars$mpg)
mean(mtcars$mpg)
min(mtcars$mpg)
max(mtcars$mpg)
sd(mtcars$mpg)

#Histograms
hist(mtcars$mpg, main="Histogram of Miles per Gallon", col = "darkblue")

#Boxplots
boxplot(mtcars$mpg)
boxplot(mtcars)

#Customise box plot
keep.par <- par()  #par(keep.par)
par(mar=c(10,4,4,2)+0.1)
boxplot(mtcars,las=3)

#Plot 
plot(mtcars$disp, mtcars$mpg, col=(mtcars$gear+1))
plot(mtcars$disp, mtcars$mpg, cex = mtcars$disp/100, col=mtcars$gear)

#Anscombe's quartet

#Anscombe's quartet is a set of four small datasets, constructed to show the importance of 
#visualising data and the dangers of reliance on simple summary statistics.

library(gridExtra)
library(ggplot2)
data(anscombe)
summary(anscombe)

# correlation
sapply(1:4, function(x) cor(anscombe[, x], anscombe[, x+4]))

# variance
sapply(5:8, function(x) var(anscombe[, x]))

# linear regression (first pair only)


lm(y1 ~ x1, data = anscombe)

p1 <- ggplot(anscombe) + geom_point(aes(x1, y1), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 1")
p2 <- ggplot(anscombe) + geom_point(aes(x2, y2), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 2")
p3 <- ggplot(anscombe) + geom_point(aes(x3, y3), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 3")
p4 <- ggplot(anscombe) + geom_point(aes(x4, y4), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 4")

grid.arrange(p1, p2, p3, p4, top = "Anscombe's Quartet")




#T-test
#The basic idea behind t-test is the inference problem from a small sample size data set to test 
#whether its sample mean may have large deviation from the true population mean.

#A very common problem you will encounter is having two data sets and you want to test whether 
#the two sets are coming from the same (assuming) normal distributions.

#In t-test, the null hypothesis is that the mean of the two samples is equal. This means that the
#alternative hypothesis for the test is that the difference of the mean is not equal to zero. 
#In a hypothesis test, we want to reject or accept the null hypothesis with some confidence 
#interval. Since we test the difference between the two means, the confidence interval in this 
#case specifies the range of values within which the difference may lie.

#The t-test will also produce the p-value, which is the probability of wrongly rejecting the null
#hypothesis. The p-value is always compared with the significance level of the test. 
#For instances, at 95% level of confidence, the significant level is 5% and the p-value is 
#reported as p<0.05. Small p-values suggest that the null hypothesis is unlikely to be true. 
#The smaller it is, the more confident we can reject the null hypothesis.


#Use the sleep data from R where there are 20 samples in two groups (group 1 and 2, each with 
#10 samples) that show the effect of two soporific drug to increase the hours in sleep
sleep
?sleep
plot(extra ~ group, data = sleep)

#there is naturally an overlap but the mean (half of the rectangle height) is different. 
#Can we confidently say that the two groups have different means?

t.test(extra ~ group, data=sleep)

#Based on the result, you can say: at 95% confidence level, there is no significant difference 
#(p-value = 0.0794) of the two means. Here you should accept the null hypothesis that the two 
#means are equal because the p-value is larger than 0.05.  The maximum difference of the mean 
#can be as low as -3.37 and as high as 0.21.  


#Z-Test

#Z-Distribution

?dnorm()  #Density, distribution function, quantile function and random generation 

#You will find the same basic functions for other distributions you may need, such as
?dt() ; ?dpois() ; ?dbinom() ; ?dchisq()

#Draw a z curve, a density function for z, Z is a variable that is normally distributed 
#with the mean 0 and the standard deviation 1

#Create a sequence of numbers for your x axis , -4 to 4 with interval 0.01
x<-seq(-4, 4, 0.01) 

#plot for each x the probability to draw x at random from the distribution
plot(x, dnorm(x), type="l")

#You can figure out what the attribute type= does by removing it 
plot(x, dnorm(x))
curve(dnorm(x), from= -4, to=4)

#Or by looking up ?plot()

#Distribution function, or Cumulative distribution function.
#For each x, what is the probability to draw x or a value lower than x
plot(x, pnorm(x))

#Quantile function is when you ask what value for z is at the 25% percentile. 
qnorm(0.25)

#You may want to find the interval of values for z that includes 95% of the distribution.
#Useful for calculating confidence intervals 
qnorm(c(0.025, 0.975))

#round the results, not for calculations but foe presentation. 
round(qnorm(c(0.025, 0.975)),2)

#it is very useful to draw graphs to enchance understanding.
#practice using the functions polygon() and text() to add clarity to your graphs.

#Plot blood pressure

#z-tests
bt <- seq(60, 120, 1) 
bt

#assume mean to be 90 and sd=10
plot(bt, dnorm(bt, 90, 10), type="l", xlim=c(60, 120), main="blood pressure") 

#Let's use the normal distribution for a statistical test.

#one tailed test
plot(bt, dnorm(bt, 90, 10), type="l", xlim=c(60, 120), main="one tailed test") 
pnorm(72, 90, 10) # probability of randomly selecting a subject bt 72 or lower
abline(v=72) # Draw a line for 72 . v is the x-value for a vertical line 
cord.x <- c(60,seq(60,72,1),72) 
cord.y <- c(0,dnorm(seq(60, 72, 1), 90, 10),0) 
polygon(cord.x,cord.y,col='skyblue') 
text(70, 0.005, "blue area = p = 0.0359") #got p value from pnorm(72, 90, 10)

#if you measure the blood pressure of a person and it turns out to be 72 then it's 
#quite possible that this person belongs to the population with the mean 90 and the 
#standard deviation 10 For this event to happen to draw a person that deviates to 72 
#or even lower blood pressure the probability that this would happen is 0.0359


#if you draw a random sample you are interested in not if this person has a blood pressure lower 
#than the certain mean, but what you are interested is whether this blood pressure deviates a certain amount 
#from the mean

#two-tailed test
bt <- seq(60, 120, 1) 
plot(bt, dnorm(bt, 90, 10), type="l", xlim=c(60, 120), main="two-tailed test") 
pnorm(72, 90, 10)  
abline(v=72) 
cord.x <- c(60,seq(60,72,1),72) 
cord.y <- c(0,dnorm(seq(60, 72, 1), 90, 10),0) 
polygon(cord.x,cord.y,col='skyblue') 

cord.x1 <- c(108,seq(108,120,1),120) 
cord.y1 <- c(0,dnorm(seq(108, 120, 1), 90, 10),0) 
polygon(cord.x1,cord.y1,col='skyblue') 
text(65, 0.005, round(pnorm(72, 90, 10), 3)) 
text(115, 0.005, round(pnorm(72, 90, 10), 3)) 
text(75, 0.02,  " p = 0.072 "  ) 

#the p-value for this two-tailed test is 36 + 36, 72. So the p-value is 0.072.
#This is the probability that you would randomly draw a person from the population 
#that deviates 18 units up or down from the mean 90


#Z, the standard normal distribution
#it allows us to take a sample and then describe how much it deviates from a hypothesized population

#By looking up the area under the Z curve from your measurement and then further away from 
#the mean you will be able to tell how common it would be to to take such a sample
#or a more extreme sample. This area is also referred to as the p-value

#If the sample deviates a lot from the mean we are going to reject the null hypothesis
#and state that this sample probably belongs to some other distribution with a different mean.
#This is called the alternative hypothesis. A common choice is to reject
#the null hypotheses if the p-value is lower than 0.05


#Using the Z distribution to decide whether a sample belongs to a population or not is actually
#a hypothesis test and as such it's called the Z test

###################################################################################################################################
#####3. LINEAR REGRESSION#####
###################################################################################################################################

#Using R dataset - mtcars
mtData <- mtcars

####Exploring Dataset####

#Checking the strcuture
str(mtData)

#Checking the top 5 rows
head(mtData,5)

####Building Model####

#Run Linear regression predicting mpg, miles per gallon using only horsepower variable
linRegHp<- lm(mpg~hp, data=mtData)
plot(mtData$hp,mtData$mpg)
abline(linRegHp)

#Decoding linRegHp
summary(linRegHp)


#R-squared = Explained variation / Total variation
#R-squared is always between 0 and 100%:
#0% indicates that the model explains none of the variability of the response data around its mean.
#100% indicates that the model explains all the variability of the response data around its mean.


###Polynomial regression###

linRegHp2<- lm(mpg~poly(hp,2, raw = TRUE), data=mtData)
summary(linRegHp2)
plot(mtData$hp,mtData$mpg)
plot(x =mtData$hp, y = linRegHp2$fitted.values)
plot(x=mtData$mpg, y=linRegHp2$fitted.values)


#Run Linear regression predicting mpg, miles per gallon using all independent varaibles
linReg<- lm(mpg~., data=mtData)

#Decoding linReg
summary(linReg)

#Predicting mpg values on original dataset 
predictions<- predict(linReg,mtData)
predictions

#####Corelation####


#Compute corelation matrix
install.packages("corrplot")
library(corrplot)
corMat <- cor(mtData)
round(corMat,2)
corrplot(corMat, method="circle")

#Converting am to categorical variable
mtData$am <- as.factor(mtData$am)
str(mtData)

#Remove cyl as feature because highly corelated with disp, and has very large p-value 
linReg<- lm(mpg~disp+hp+drat+wt+qsec+vs+am+gear+carb, data=mtData)
linReg<- lm(mpg~.-cyl, data=mtData)

summary(linReg)

plot(mtData$mpg,linReg$fitted.values)

###Analyse resiudal plots, QQ plots etc of Linear Regression###

#Get all 4 plots in one screen
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(linReg)
par(mfrow=c(1,1)) # Change back to 1 x 1


####Splitting Dataset####

#Splitting data 
# Load the package:
library(caTools)
# Set our seed so we all get the same split
set.seed(13)

# Randomly split the data: specify the dependent/target variable and the split ratio
spl = sample.split(mtData$mpg, SplitRatio = 0.65)
trainData = subset(mtData, spl == TRUE)
testData = subset(mtData, spl == FALSE)

#Build model on train data set
linReg<- lm(mpg~., data=trainData)
summary(linReg)

####Prediction and accuracy measurement####

#predicting on train
predictionsTrain<- predict(linReg,trainData)

#Error percentage on train
errorPTrain <- abs(trainData$mpg - predictionsTrain)*100/trainData$mpg
100-mean(errorPTrain)

#predicting on test
predictionsTest<- predict(linReg,testData)

#Error percentage on train
errorPTest <- abs(testData$mpg - predictionsTest)*100/testData$mpg
100-mean(errorPTest)

###################################################################################################################################
#####4. LOGISTIC REGRESSION#####
###################################################################################################################################

install.packages("mlbench")
install.packages("GGally")
install.packages("pROC")
install.packages("caret")
install.packages("caTools")
install.packages("ggplot2")
install.packages("plotly")


library(mlbench)
library(plotly)

#Get the data in the environment
data("PimaIndiansDiabetes")
  
#Store data in a variable
piData<-PimaIndiansDiabetes

#Dataset description
?PimaIndiansDiabetes

#Checking the strcuture
str(piData)

#Checking the top 5 rows
head(piData)

#explore data

library(ggplot2)
library(GGally)

ggpairs(piData)

#Splitting data 
# Load the package:
library(caTools)
# Set our seed so we all get the same split
set.seed(13)
# Randomly split the data:
spl = sample.split(piData$diabetes, SplitRatio = 0.7)
trainData = subset(piData, spl == TRUE)
testData = subset(piData, spl == FALSE)

#Run Logistic regression predicting probability of occurence of diabetes
library(caret)
logReg<- glm(diabetes~., data=trainData, family=binomial)

#checking variable importance of each independent variable learnt from the model
varImp(logReg)

#Summarize logistic regresion
summary(logReg)

#Checking Model's Predictions (probability estimates) on train Data
predicted_prob<-predict(logReg, trainData, type="response")
predicted_prob

#appends predicted vals to the test data and stores in a separate data frame
prediction_data<-data.frame(trainData,predicted_prob)
head(prediction_data)

#converting probabilities to pos/neg tag with 0.5 threshold
prediction_class<-ifelse(predicted_prob<0.5,"neg","pos")
prediction_class

#Confusion Matrix
#table(data = prediction_class, reference = testData$diabetes)
xtab<- table(  trainData$diabetes,prediction_class)
xtab

#Calculating accuracy i.e. proportion of correctly predicted instances accuracy=77.32%
confusionMatrix(xtab, positive = "pos")

#plotting roc curve
library(pROC)
rocCurve <- roc(diabetes ~ predicted_prob, data = trainData)
plot(rocCurve)

#Area under curve, auc = 0.8438
auc(rocCurve)

#vary threshold level

#vary threshold from 0.1-0.9 , step size=0.1
cutoffs <- seq(0.1,0.9,0.1)
accuracy <- NULL

#logging accuracy for each threshold level
for (i in seq(along = cutoffs)){
  prediction <- ifelse(logReg$fitted.values >= cutoffs[i], "pos", "neg") #Predicting for cut-off
  accuracy <- c(accuracy,length(which(trainData$diabetes ==prediction))/length(prediction)*100)
}


#Plotting varying accuracy by varying threshold
plot(cutoffs, accuracy, pch =19,type='b',col= "steelblue",
     main ="Logistic Regression", xlab="Cutoff Level", ylab = "Accuracy %")


#checking confusion matrix with 0.6 threshold, accuracy = 79% (accuracy increases with change in threshold from 0.5 to 0.6)
prediction_class<-ifelse(predicted_prob>=0.6,"pos","neg")
xtabNew<- table(trainData$diabetes, prediction_class)
confusionMatrix(xtabNew,positive = "pos")

#Plot most significant variable vs diabetes
boxplot(piData$glucose~piData$diabetes)
boxplot(piData$mass~ piData$diabetes)
boxplot(piData$pregnant~ piData$diabetes)

###################################################################################################################################
#####5. DECISION TREE#####
###################################################################################################################################

install.packages("MASS")
install.packages("tree")
install.packages("partykit")
install.packages("Amelia")

library(MASS)
library(tree)
library(caTools)
library(rpart)
library(mlbench)
library(partykit)


?Boston
bostonHousing<- Boston
head(bostonHousing)

#check for na's
library(Amelia)
missmap(bostonHousing,col=c('yellow','lightblue'),y.at=1,y.labels='',legend=TRUE)

#explore data

library(ggplot2)
library(GGally)
ggpairs(bostonHousing)


library(dplyr)
library(reshape2)
bostonHousing %>%
  select(c(crim, rm, age, rad, tax, lstat, medv,indus,nox,ptratio,zn)) %>%
  melt(id.vars = "medv") %>%
  ggplot(aes(x = value, y = medv, colour = variable)) +
  geom_point(alpha = 0.7) +
  stat_smooth(aes(colour = "black")) +
  facet_wrap(~variable, scales = "free", ncol = 2) +
  labs(x = "Variable Value", y = "Median House Price ($1000s)") +
  theme_minimal()

lr<- lm(medv~poly(lstat,2, raw = TRUE), data=bostonHousing)
summary(lr)
plot(x = bostonHousing$lstat, y = lr$fitted.values)
plot(lr)

#Splitting data 
# Load the package:
library(caTools)
# Set our seed so we all get the same split
set.seed(13)
# Randomly split the data:
spl = sample.split(bostonHousing$medv, SplitRatio = 0.65)
trainData = subset(bostonHousing, spl == TRUE)
testData = subset(bostonHousing, spl == FALSE)

#Creating the tree 

# grow tree 
fit <- rpart(medv~.,method="anova", data=trainData)
#fit <- lm(medv~.,data=trainData)

printcp(fit) # display the results 
plotcp(fit) # visualize cross-validation results 
summary(fit) # detailed summary of splits
print(fit)


#Plotting the tree
library(rpart.plot)
prp(type=2,extra=1,digits=4,box.palette="auto",fallen.leaves = TRUE,fit)
#fancyRpartPlot(fit)
plot(as.party(fit))
library(rattle)
fancyRpartPlot(fit,main=paste('RPART:'))

library(e1071)
## tune `rpart' for regression, using 10-fold cross validation (default)
fitRpartTune <- tune.rpart(medv~., data = trainData, minsplit = c(5,10,15,20,25,30,35))
summary(fitRpartTune)
plot(fitRpartTune)

prp(type=2,extra=1,digits=4,box.palette="auto",fallen.leaves = TRUE,fitRpartTune)
#fancyRpartPlot(fitRpartTune)


####Prediction and accuracy measurement####

#predicting on train
predictionsTrain<- predict(fit,trainData)

#Error percentage on train
errorPTrain <- abs(trainData$medv - predictionsTrain)*100/trainData$medv
100-mean(errorPTrain)

#predicting on test
predictionsTest<- predict(fit,testData)

#Error percentage on train
errorPTest <- abs(testData$medv - predictionsTest)*100/testData$medv
100-mean(errorPTest)

# plot tree 
plot(fit, uniform=TRUE, 
     main="Regression Tree for Boston Housing ")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

###################################################################################################################################
#####6. RANDOM FOREST#####
###################################################################################################################################

require(randomForest)
require(MASS)#Package which contains the Boston housing dataset

dim(Boston)
?Boston
bostonHousing<- Boston
head(bostonHousing)

#check for na's
library(Amelia)
missmap(bostonHousing,col=c('yellow','lightblue'),y.at=1,y.labels='',legend=TRUE)

#explore data

library(ggplot2)
library(GGally)
ggpairs(bostonHousing)

library(dplyr)
library(reshape2)
bostonHousing %>%
  select(c(crim, rm, age, rad, tax, lstat, medv,indus,nox,ptratio,zn)) %>%
  melt(id.vars = "medv") %>%
  ggplot(aes(x = value, y = medv, colour = variable)) +
  geom_point(alpha = 0.7) +
  stat_smooth(aes(colour = "black")) +
  facet_wrap(~variable, scales = "free", ncol = 2) +
  labs(x = "Variable Value", y = "Median House Price ($1000s)") +
  theme_minimal()

#Splitting data 
# Load the package:
library(caTools)
# Set our seed so we all get the same split
set.seed(13)
# Randomly split the data:
spl = sample.split(bostonHousing$medv, SplitRatio = 0.65)
trainData = subset(bostonHousing, spl == TRUE)
testData = subset(bostonHousing, spl == FALSE)

#-------------------------------------------------------------------------------------#
Boston.rf<-randomForest(medv ~ . , data = trainData, importance=TRUE)
Boston.rf

plot(Boston.rf)

#Variable importance
importance(Boston.rf)
VI_F=importance(Boston.rf)
VI_F

# represents the mean decrease in node impurity
#library(caret)

varImpPlot(Boston.rf)
varImpPlot(Boston.rf,type = 2)

####Prediction and accuracy measurement####

#Error percentage on train , ~95% accuracy
predictionsTrain<- predict(Boston.rf,trainData)
errorPTrain <- abs(trainData$medv - predictionsTrain)*100/trainData$medv
100-mean(errorPTrain)

#Error percentage on test, ~88% accuracy
predictionsTest<- predict(Boston.rf,testData)
errorPTest <- abs(testData$medv - predictionsTest)*100/testData$medv
100-mean(errorPTest)

#####################################################################################

library(caret)
library(e1071)
## tune `random forest' for regression, using 10-fold cross validation (default)
fitRFTune <- tune.randomForest(medv~., data = testData,ntree = c(3,5,7,11,15,25,40))
summary(fitRFTune)
plot(fitRFTune)

fitRFTune2 <- tune.randomForest(medv~., data = testData, mtry=c(3,5,7,11))
summary(fitRFTune2)
plot(fitRFTune2)

#Tune multiple parameters together
fitRFTune3 <- tune.randomForest(medv~., data = testData, mtry=c(3,5,7,11),ntree = c(25,40,50,100,200,400,1500))
summary(fitRFTune3)
plot(fitRFTune3)

#####################################################################################


#Grid Search
#Another search is to define a grid of algorithm parameters to try.

#Each axis of the grid is an algorithm parameter, and points in the grid are specific combinations of 
#parameters. Because we are only tuning one parameter, the grid search is a linear search through a vector 
#of candidate values.
metric <- "Accuracy" #for classification models
metric <- "RMSE" # for regression models
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(13)

tunegrid <- expand.grid(.mtry=c(1:13))
rf_gridsearch <- train(medv~., data=trainData, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)

############################################################################################

#The above Random Forest model chose Randomly 4 variables to be considered at each split. We could now try all possible 13 predictors which can be found at each split.
oob.err<-double(13)
test.err<-double(13)

#mtry is no of Variables randomly chosen at each split
for(mtry in 1:13) 
{
  rf=randomForest(medv ~ . , data = trainData,mtry=mtry,ntree=40) 
  oob.err[mtry] = rf$mse[40] #Error of all Trees fitted
  
  pred<-predict(rf,testData) #Predictions on Test Set for each Tree
  test.err[mtry]= with(testData, mean( (medv - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ")
  
}

test.err 
oob.err

#Comparing both Test Error and Out of Sample Estimation for Random Forests
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))

#---------------------------------------------------------------------------------------#
rf_model <- randomForest(medv ~ ., data=trainData,
                         mtry=11,
                         ntree=40,     # number of trees in the Random Forest
                         nodesize=25, # minimum node size set small enough to allow for complex trees,
                         # but not so small as to require too large B to eliminate high variance
                         keep.inbag=TRUE, importance=TRUE)

###################################################################################################################################
#####7. CLUSTERING#####
###################################################################################################################################

library(ggfortify)

#Load iris data into a dataframe
df <- iris[c(1, 2, 3, 4)]

#Do PCA on the data frame and plot the first 2 components
autoplot(prcomp(df))

#Plot the PCA by color coding the data points by the flower species
autoplot(prcomp(df), data = iris, colour = 'Species')

#Labeling the data points with record number
autoplot(prcomp(df), data = iris, colour = 'Species', label = TRUE, label.size = 3)

autoplot(prcomp(df), data = iris, colour = 'Species', label = FALSE, label.size = 3)

#Plot the loadings of features learnt from PCA
autoplot(prcomp(df), data = iris, colour = 'Species', loadings = TRUE)

#Plotting all the information simultaneously
autoplot(prcomp(df), data = iris, colour = 'Species',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)

#Clustering on principal components of Iris Data
library(cluster)

autoplot(clara(iris[-5], 3))

#Clustering with 3 clusters and visualizing the cluster assignment
autoplot(fanny(iris[-5], 3), frame = TRUE)

#Clustering with n=3 and visualizing clusters with different frame
autoplot(pam(iris[-5], 3), frame = TRUE, frame.type = 'norm')


###################################################################################################################################
#####8. PCA#####
###################################################################################################################################

#Use dataset USArrests
?USArrests
str(USArrests)
head(USArrests)
names(USArrests)
states=row.names(USArrests)
states

#Find means of all columns

#Note that the apply() function allows us to apply a function-in this case,
#the mean() function-to each row or column of the data set. The second
#input here denotes whether we wish to compute the mean of the rows, 1,
#or the columns, 2

apply(USArrests,2,mean)

#We notice that the variables have vastly different means.

#Analyse the variance
apply(USArrests,2,var)

#perform principal components analysis using the prcomp() function
pr.out=prcomp(USArrests , scale=TRUE)

names(pr.out)

#By default, the prcomp() function centers the variables to have mean zero.
#By using the option scale=TRUE, we scale the variables to have standard
#deviation one.

#The center and scale components correspond to the means and standard
#deviations of the variables that were used for scaling prior to implementing PCA

pr.out$center
pr.out$scale

#The rotation matrix provides the principal component loadings; each column
#of pr.out$rotation contains the corresponding principal component
#loading vector

pr.out

pr.out$rotation

dim(pr.out$x)
pr.out$x

#We can plot the first two principal components as follows:
biplot (pr.out , scale =0) 

#The scale=0 argument to biplot() ensures that the arrows are scaled to biplot() represent the 
#loadings; other values for scale give slightly different biplots with different interpretations

pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot (pr.out , scale =0)

#e first loading vector places approximately equal weight on Assault, Murder, and Rape, 
#with much less weight on UrbanPop. Hence this component roughly corresponds to a measure of overall
#rates of serious crimes. The second loading vector places most of its weight
#on UrbanPop and much less weight on the other three features. Hence, this
#component roughly corresponds to the level of urbanization of the state

#crime-related variables are correlated
#with each other-states with high murder rates tend to have high
#assault and rape rates-and that the UrbanPop variable is less correlated
#with the other three.

#states with large positive scores on the first component,
#such as California, Nevada and Florida, have high crime rates, while
#states like North Dakota, with negative scores on the first component, have
#low crime rates. California also has a high score on the second component,
#indicating a high level of urbanization, while the opposite is true for states
#like Mississippi. States close to zero on both components, such as Indiana,
#have approximately average levels of both crime and urbanization

#The prcomp() function also outputs the standard deviation of each principal component
pr.out$sdev

#The variance explained by each principal component is obtained by squaring
pr.var=pr.out$sdev ^2
pr.var

#To compute the proportion of variance explained by each principal component,
#we simply divide the variance explained by each principal component
#by the total variance explained by all four principal components:

pve=pr.var/sum(pr.var)
pve

#We see that the first principal component explains 62.0 % of the variance
#in the data, the next principal component explains 24.7 % of the variance,
#and so forth. We can plot the PVE explained by each component, as well
#as the cumulative PVE, as follows:

plot(pve , xlab=" Principal Component ", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component ", ylab="
       Cumulative Proportion of Variance Explained ", ylim=c(0,1),
       type='b')


##########################################################################

###CLUSTERING###
library(cluster)
library(HSAUR)
library(fpc)

# Kmeans on original PCA of USArrests
clust <- kmeans(pr.out$x[,1:2], centers=2)
clusplot(pr.out$x[,1:2], clust$cluster, color=TRUE, shade=TRUE, 
         labels=2, lines=0)
#########################################################################